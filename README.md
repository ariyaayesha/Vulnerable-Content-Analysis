# ğŸ›¡ï¸ Vulnerable Content Analysis System

A Machine Learningâ€“based system to detect **vulnerable, harmful, and bullying content** in text data.  
This project uses **NLP techniques and a trained ML model** to classify text and provides a simple **web interface** for real-time prediction.

---

## ğŸ“Œ Project Overview

Online platforms often face challenges in identifying **toxic, abusive, or bullying content**.  
This project aims to automatically detect such vulnerable content using **Natural Language Processing (NLP)** and **Machine Learning**.

The system:
- Analyzes text input
- Extracts features using **TF-IDF**
- Predicts whether the content is **bullying/vulnerable or safe**
- Displays results via a **Flask web application**

---

## ğŸš€ Features

- ğŸ§  Machine Learningâ€“based text classification  
- ğŸ“Š TF-IDF feature extraction  
- ğŸŒ Web interface using Flask  
- âš¡ Real-time text prediction  
- ğŸ’¾ Pre-trained model for fast inference  

---

## ğŸ§° Technologies Used

- **Python**
- **Flask**
- **Scikit-learn**
- **NLTK**
- **Pandas & NumPy**
- **Pickle**

---

## ğŸ“‚ Project Structure


<img width="671" height="207" alt="image" src="https://github.com/user-attachments/assets/6592e34d-88f4-47cd-ba82-b430fb29ae5e" />



---

## Result


<img width="612" height="184" alt="image" src="https://github.com/user-attachments/assets/6290a1d6-9198-42e4-90b8-5f5c06e8cc27" />


<img width="575" height="502" alt="image" src="https://github.com/user-attachments/assets/d8037e36-63de-4f36-a699-fae7794eeeff" />

<img width="995" height="493" alt="image" src="https://github.com/user-attachments/assets/4513384c-0a18-4cf6-ab4c-979cfc25f6ee" />

<img width="994" height="529" alt="image" src="https://github.com/user-attachments/assets/cdd4cc47-7c73-4114-9a53-ba7c26bf96e5" />

<img width="979" height="509" alt="image" src="https://github.com/user-attachments/assets/ac2df6e5-5ca5-49ee-bd8c-7a11af9900c2" />





## âš™ï¸ How It Works

1. User enters a text message
2. Text is cleaned and transformed using **TF-IDF**
3. Pre-trained ML model predicts the content type
4. Output is displayed as:
   - **Bullying / Vulnerable Content**
   - **Safe Content**

---


